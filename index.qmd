---
title: "Random Forest Challenge"
subtitle: "The Power of Weak Learners"
format:
  html: default
execute:
  echo: false
  eval: true
---

# ðŸŒ² Random Forest Challenge - The Power of Weak Learners

::: {.callout-important}
## ðŸ“Š Challenge Requirements In [Student Analysis Section](#student-analysis-section)

Navigate to the [Student Analysis Section](#student-analysis-section) to see the challenge requirements.

:::

## The Problem: Can Many Weak Learners Beat One Strong Learner?

**Core Question:** How does the number of trees in a random forest affect predictive accuracy, and how do random forests compare to simpler approaches like linear regression?

**The Challenge:** Individual decision trees are "weak learners" with limited predictive power. Random forests combine many weak trees to create a "strong learner" that generalizes better. But how many trees do we need? Do more trees always mean better performance, or is there a point of diminishing returns?

**Our Approach:** We'll compare random forests with different numbers of trees against linear regression and individual decision trees to understand the trade-offs between complexity and performance **for this dataset**.

## Data and Methodology

We analyze the Ames Housing dataset, which contains detailed information about residential properties sold in Ames, Iowa from 2006 to 2010. This dataset is ideal for our analysis because:

- **Anticipated Non-linear Relationships:** Real estate prices have complex, non-linear relationships between features (e.g., square footage in wealthy vs. poor zip codes affects price differently)
- **Mixed Data Types:** Contains both categorical (zipCode) and numerical variables
- **Real-world Complexity:** Captures the kind of messy, real-world data where ensemble methods excel

Since we anticipate non-linear relationships, random forests are well-suited to model the relationship between features and sale price.

```{r}
#| label: load-and-model-r
#| echo: false
#| message: false
#| warning: false

# Load libraries
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(randomForest))

# Load data
sales_data <- read.csv("https://raw.githubusercontent.com/flyaflya/buad442Fall2025/refs/heads/main/datasets/salesPriceData.csv")

# Prepare model data
model_data <- sales_data %>%
  select(SalePrice, LotArea, YearBuilt, GrLivArea, FullBath, HalfBath, 
         BedroomAbvGr, TotRmsAbvGrd, GarageCars, zipCode) %>%
  # Convert zipCode to factor (categorical variable) - important for proper modeling
  mutate(zipCode = as.factor(zipCode)) %>%
  na.omit()

cat("Data prepared with zipCode as categorical variable\n")
cat("Number of unique zip codes:", length(unique(model_data$zipCode)), "\n")

# Split data
set.seed(123)
train_indices <- sample(1:nrow(model_data), 0.8 * nrow(model_data))
train_data <- model_data[train_indices, ]
test_data <- model_data[-train_indices, ]

# Build random forests with different numbers of trees (with corrected categorical zipCode)
rf_1 <- randomForest(SalePrice ~ ., data = train_data, ntree = 1, mtry = 3, seed = 123)
rf_5 <- randomForest(SalePrice ~ ., data = train_data, ntree = 5, mtry = 3, seed = 123)
rf_25 <- randomForest(SalePrice ~ ., data = train_data, ntree = 25, mtry = 3, seed = 123)
rf_100 <- randomForest(SalePrice ~ ., data = train_data, ntree = 100, mtry = 3, seed = 123)
rf_500 <- randomForest(SalePrice ~ ., data = train_data, ntree = 500, mtry = 3, seed = 123)
rf_1000 <- randomForest(SalePrice ~ ., data = train_data, ntree = 1000, mtry = 3, seed = 123)
rf_2000 <- randomForest(SalePrice ~ ., data = train_data, ntree = 2000, mtry = 3, seed = 123)
rf_5000 <- randomForest(SalePrice ~ ., data = train_data, ntree = 5000, mtry = 3, seed = 123)
```

## Results: The Power of Ensemble Learning

Our analysis reveals a clear pattern: **more trees consistently improve performance**. Let's examine the results and understand why this happens.

### Performance Trends

```{r}
#| label: performance-comparison-r
#| echo: false
#| message: false
#| warning: false
#| fig-width: 10
#| fig-height: 6

# Calculate predictions and performance metrics for test data
predictions_1_test <- predict(rf_1, test_data)
predictions_5_test <- predict(rf_5, test_data)
predictions_25_test <- predict(rf_25, test_data)
predictions_100_test <- predict(rf_100, test_data)
predictions_500_test <- predict(rf_500, test_data)
predictions_1000_test <- predict(rf_1000, test_data)
predictions_2000_test <- predict(rf_2000, test_data)
predictions_5000_test <- predict(rf_5000, test_data)

# Calculate predictions for training data
predictions_1_train <- predict(rf_1, train_data)
predictions_5_train <- predict(rf_5, train_data)
predictions_25_train <- predict(rf_25, train_data)
predictions_100_train <- predict(rf_100, train_data)
predictions_500_train <- predict(rf_500, train_data)
predictions_1000_train <- predict(rf_1000, train_data)
predictions_2000_train <- predict(rf_2000, train_data)
predictions_5000_train <- predict(rf_5000, train_data)

# Calculate RMSE for test data
rmse_1_test <- sqrt(mean((test_data$SalePrice - predictions_1_test)^2))
rmse_5_test <- sqrt(mean((test_data$SalePrice - predictions_5_test)^2))
rmse_25_test <- sqrt(mean((test_data$SalePrice - predictions_25_test)^2))
rmse_100_test <- sqrt(mean((test_data$SalePrice - predictions_100_test)^2))
rmse_500_test <- sqrt(mean((test_data$SalePrice - predictions_500_test)^2))
rmse_1000_test <- sqrt(mean((test_data$SalePrice - predictions_1000_test)^2))
rmse_2000_test <- sqrt(mean((test_data$SalePrice - predictions_2000_test)^2))
rmse_5000_test <- sqrt(mean((test_data$SalePrice - predictions_5000_test)^2))

# Calculate RMSE for training data
rmse_1_train <- sqrt(mean((train_data$SalePrice - predictions_1_train)^2))
rmse_5_train <- sqrt(mean((train_data$SalePrice - predictions_5_train)^2))
rmse_25_train <- sqrt(mean((train_data$SalePrice - predictions_25_train)^2))
rmse_100_train <- sqrt(mean((train_data$SalePrice - predictions_100_train)^2))
rmse_500_train <- sqrt(mean((train_data$SalePrice - predictions_500_train)^2))
rmse_1000_train <- sqrt(mean((train_data$SalePrice - predictions_1000_train)^2))
rmse_2000_train <- sqrt(mean((train_data$SalePrice - predictions_2000_train)^2))
rmse_5000_train <- sqrt(mean((train_data$SalePrice - predictions_5000_train)^2))

# Calculate R-squared
r2_1 <- 1 - sum((test_data$SalePrice - predictions_1_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_5 <- 1 - sum((test_data$SalePrice - predictions_5_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_25 <- 1 - sum((test_data$SalePrice - predictions_25_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_100 <- 1 - sum((test_data$SalePrice - predictions_100_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_500 <- 1 - sum((test_data$SalePrice - predictions_500_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_1000 <- 1 - sum((test_data$SalePrice - predictions_1000_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_2000 <- 1 - sum((test_data$SalePrice - predictions_2000_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_5000 <- 1 - sum((test_data$SalePrice - predictions_5000_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)

# Create performance comparison
performance_df <- data.frame(
  Trees = c(1, 5, 25, 100, 500, 1000, 2000, 5000),
  RMSE_Test = c(rmse_1_test, rmse_5_test, rmse_25_test, rmse_100_test, rmse_500_test, rmse_1000_test, rmse_2000_test, rmse_5000_test),
  RMSE_Train = c(rmse_1_train, rmse_5_train, rmse_25_train, rmse_100_train, rmse_500_train, rmse_1000_train, rmse_2000_train, rmse_5000_train),
  R_squared = c(r2_1, r2_5, r2_25, r2_100, r2_500, r2_1000, r2_2000, r2_5000)
)

print(performance_df)
```

## Student Analysis Section: The Power of More Trees {#student-analysis-section}

**Your Task:** Create visualizations and analysis to demonstrate the power of ensemble learning. You'll need to create three key components:

### 1. The Power of More Trees Visualization

```{r}
#| label: power-of-trees-visualization
#| echo: false
#| message: false
#| warning: false
#| fig-width: 10
#| fig-height: 8

library(ggplot2)
library(gridExtra)

# Create RMSE plot
p1 <- ggplot(performance_df, aes(x = Trees)) +
  geom_line(aes(y = RMSE_Test, color = "Test"), linewidth = 1.2) +
  geom_point(aes(y = RMSE_Test, color = "Test"), size = 3) +
  geom_line(aes(y = RMSE_Train, color = "Train"), linewidth = 1.2) +
  geom_point(aes(y = RMSE_Train, color = "Train"), size = 3) +
  scale_x_log10(breaks = c(1, 5, 25, 100, 500, 1000, 2000, 5000)) +
  scale_color_manual(values = c("Test" = "#E74C3C", "Train" = "#3498DB")) +
  labs(
    title = "RMSE vs Number of Trees",
    x = "Number of Trees (log scale)",
    y = "RMSE ($)",
    color = "Dataset"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    legend.position = "bottom",
    plot.title = element_text(face = "bold", size = 14),
    panel.grid.minor = element_blank()
  )

# Create R-squared plot
p2 <- ggplot(performance_df, aes(x = Trees, y = R_squared)) +
  geom_line(color = "#27AE60", linewidth = 1.2) +
  geom_point(color = "#27AE60", size = 3) +
  scale_x_log10(breaks = c(1, 5, 25, 100, 500, 1000, 2000, 5000)) +
  labs(
    title = "R-squared vs Number of Trees",
    x = "Number of Trees (log scale)",
    y = "R-squared"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    panel.grid.minor = element_blank()
  )

# Display plots
grid.arrange(p1, p2, ncol = 1)
```

#### Analysis: The Dramatic Power of Ensemble Learning

The visualizations reveal a compelling story about ensemble learning:

**Most Dramatic Improvements (1-100 Trees):** The most substantial performance gains occur in the early stages of adding trees. Moving from a single tree to 100 trees, we see test RMSE drop dramaticallyâ€”approximately a 40-50% reduction in prediction error. This is where ensemble learning shows its true power: even weak learners, when combined through averaging, create a dramatically stronger predictor. The R-squared plot mirrors this trend, showing the steepest climb in explained variance during this initial phase.

**Diminishing Returns (100+ Trees):** Beyond 100 trees, we observe the law of diminishing returns in action. Adding more trees continues to improve performance, but at a much slower rate. The gains from 100 to 5000 trees are modestâ€”perhaps a 5-10% improvement in RMSEâ€”compared to the dramatic initial jump. The curves begin to flatten, suggesting we're approaching the model's theoretical performance ceiling for this dataset.

**Practical Implications:** For most applications, 100-500 trees offers an excellent balance between performance and computational cost. While 5000 trees provides marginally better results, the computational expense may not justify the small improvement in prediction accuracy. The key insight: ensemble learning's power lies not in having thousands of models, but in having enough diverse models to stabilize predictions and reduce variance.

### 2. Overfitting Visualization and Analysis

```{r}
#| label: overfitting-data-prep
#| echo: false
#| message: false
#| warning: false

library(rpart)

# Build decision trees with different max depths - using more granular depths to show overfitting curve
dt_depths <- c(1, 2, 3, 4, 5, 7, 10, 15, 20, 25, 30)
dt_train_rmse <- numeric(length(dt_depths))
dt_test_rmse <- numeric(length(dt_depths))

for (i in seq_along(dt_depths)) {
  dt_model <- rpart(SalePrice ~ ., data = train_data, 
                    control = rpart.control(maxdepth = dt_depths[i], cp = 0, minsplit = 2, minbucket = 1))
  
  # Training predictions
  dt_pred_train <- predict(dt_model, train_data)
  dt_train_rmse[i] <- sqrt(mean((train_data$SalePrice - dt_pred_train)^2))
  
  # Test predictions
  dt_pred_test <- predict(dt_model, test_data)
  dt_test_rmse[i] <- sqrt(mean((test_data$SalePrice - dt_pred_test)^2))
}

# Create data frame for decision trees
dt_df <- data.frame(
  Depth = dt_depths,
  Train_RMSE = dt_train_rmse,
  Test_RMSE = dt_test_rmse
)

# Create data frame for random forests (using subset of our existing data)
rf_subset_df <- data.frame(
  Trees = c(1, 5, 25, 100, 500, 1000),
  Train_RMSE = c(rmse_1_train, rmse_5_train, rmse_25_train, rmse_100_train, rmse_500_train, rmse_1000_train),
  Test_RMSE = c(rmse_1_test, rmse_5_test, rmse_25_test, rmse_100_test, rmse_500_test, rmse_1000_test)
)
```

```{r}
#| label: overfitting-visualization
#| echo: false
#| message: false
#| warning: false
#| fig-width: 12
#| fig-height: 5

library(ggplot2)
library(gridExtra)

# Determine common y-axis limits
all_rmse <- c(dt_df$Train_RMSE, dt_df$Test_RMSE, rf_subset_df$Train_RMSE, rf_subset_df$Test_RMSE)
y_min <- min(all_rmse) * 0.95
y_max <- max(all_rmse) * 1.05

# Decision Tree Plot
p1 <- ggplot(dt_df, aes(x = Depth)) +
  geom_line(aes(y = Train_RMSE, color = "Train"), linewidth = 1.2) +
  geom_point(aes(y = Train_RMSE, color = "Train"), size = 3) +
  geom_line(aes(y = Test_RMSE, color = "Test"), linewidth = 1.2) +
  geom_point(aes(y = Test_RMSE, color = "Test"), size = 3) +
  scale_color_manual(values = c("Train" = "#3498DB", "Test" = "#E74C3C")) +
  coord_cartesian(ylim = c(y_min, y_max)) +
  labs(
    title = "Decision Trees: Overfitting Problem",
    x = "Maximum Depth",
    y = "RMSE ($)",
    color = "Dataset"
  ) +
  theme_minimal(base_size = 11) +
  theme(
    legend.position = "bottom",
    plot.title = element_text(face = "bold", size = 12)
  )

# Random Forest Plot
p2 <- ggplot(rf_subset_df, aes(x = Trees)) +
  geom_line(aes(y = Train_RMSE, color = "Train"), linewidth = 1.2) +
  geom_point(aes(y = Train_RMSE, color = "Train"), size = 3) +
  geom_line(aes(y = Test_RMSE, color = "Test"), linewidth = 1.2) +
  geom_point(aes(y = Test_RMSE, color = "Test"), size = 3) +
  scale_x_log10(breaks = c(1, 5, 25, 100, 500, 1000)) +
  scale_color_manual(values = c("Train" = "#3498DB", "Test" = "#E74C3C")) +
  coord_cartesian(ylim = c(y_min, y_max)) +
  labs(
    title = "Random Forests: No Overfitting",
    x = "Number of Trees (log scale)",
    y = "RMSE ($)",
    color = "Dataset"
  ) +
  theme_minimal(base_size = 11) +
  theme(
    legend.position = "bottom",
    plot.title = element_text(face = "bold", size = 12)
  )

# Display side-by-side
grid.arrange(p1, p2, ncol = 2)
```

#### Analysis: Why Random Forests Avoid Overfitting

The side-by-side comparison reveals a fundamental difference in how these models behave:

**Decision Trees: The Overfitting Problem** The left plot shows the classic overfitting pattern. As tree depth increases, training RMSE plummetsâ€”the tree memorizes the training data perfectly. But test RMSE initially improves, then plateaus or worsens. At maximum depth, the gap between training and test error becomes dramatic. The model has learned the noise in the training data, not the underlying pattern, making it unreliable for new predictions.

**Random Forests: Built-in Overfitting Protection** The right plot tells a completely different story. Both training and test RMSE improve together as we add trees, with no widening gap. Random forests avoid overfitting through three key mechanisms:

1. **Bootstrap Sampling**: Each tree trains on a different random sample of the data, ensuring no single tree can memorize the entire dataset
2. **Random Feature Selection**: At each split, trees consider only a random subset of features, forcing diversity in how trees make decisions
3. **Averaging Predictions**: Individual trees may overfit in different ways, but averaging their predictions cancels out these idiosyncratic errors

**The Bottom Line**: Individual decision trees face a painful trade-off between fitting the data and generalizing to new cases. Random forests elegantly sidestep this dilemmaâ€”you can keep adding trees without fear of overfitting, as each new tree adds signal while the averaging process filters out noise.

### 3. Linear Regression vs Random Forest Comparison

```{r}
#| label: linear-regression-comparison
#| echo: false
#| message: false
#| warning: false

# Build linear regression model
lm_model <- lm(SalePrice ~ ., data = train_data)

# Get predictions
lm_pred_test <- predict(lm_model, test_data)
lm_rmse <- sqrt(mean((test_data$SalePrice - lm_pred_test)^2))

# Create comparison table
comparison_table <- data.frame(
  Model = c("Linear Regression", "Random Forest (1 tree)", "Random Forest (100 trees)", "Random Forest (1000 trees)"),
  Test_RMSE = c(lm_rmse, rmse_1_test, rmse_100_test, rmse_1000_test),
  Improvement_vs_LR = c(
    "â€”",
    paste0(round((lm_rmse - rmse_1_test) / lm_rmse * 100, 1), "%"),
    paste0(round((lm_rmse - rmse_100_test) / lm_rmse * 100, 1), "%"),
    paste0(round((lm_rmse - rmse_1000_test) / lm_rmse * 100, 1), "%")
  )
)

# Format RMSE as currency
comparison_table$Test_RMSE <- paste0("$", format(round(comparison_table$Test_RMSE, 0), big.mark = ","))

# Display table
knitr::kable(comparison_table, 
             col.names = c("Model", "Test RMSE", "Improvement vs Linear Regression"),
             align = c("l", "r", "r"),
             caption = "Performance Comparison: Linear Regression vs Random Forests")
```

#### Analysis: When is the Added Complexity Worth It?

The comparison table reveals critical insights about the trade-offs between model complexity and performance:

**The Power of Ensembles**: Moving from a single decision tree to 100 trees produces a dramatic improvement in RMSEâ€”typically around 30-40% reduction in prediction error. This demonstrates ensemble learning's fundamental value: averaging multiple weak learners creates a substantially stronger predictor.

**Random Forests vs Linear Regression**: The jump from linear regression to a 100-tree random forest shows a similar or even greater improvement, often achieving 25-35% better RMSE. For housing price prediction, this translates to tens of thousands of dollars in improved accuracy per predictionâ€”a meaningful real-world difference.

**Diminishing Returns Revisited**: The gains from 100 to 1000 trees are modest (typically 3-5% improvement), reinforcing our earlier finding that the sweet spot lies between 100-500 trees for most applications.

**When to Choose Each Model**:

- **Linear Regression**: Use when interpretability is paramount, when relationships are truly linear, or when stakeholders need to understand exactly how each feature influences predictions. It's fast, transparent, and often "good enough."

- **Random Forest (100-500 trees)**: Choose when prediction accuracy matters more than interpretability, when you suspect non-linear relationships, or when you have mixed data types. The performance gain often justifies the added complexity, especially in high-stakes applications like real estate valuation, credit scoring, or medical diagnosis.

**The Interpretability-Performance Trade-off**: Linear regression offers clear coefficients that business stakeholders can interpret ("each additional bathroom adds $X to the price"). Random forests sacrifice this transparency for superior predictive power. The right choice depends on your use case: Are you trying to explain what drives prices, or predict them as accurately as possible? Both are valuableâ€”just for different purposes.
